{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5138f29",
   "metadata": {},
   "source": [
    "## run_inference Fonksiyonunda Ben Aslında Ne Yapıyorum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1cb7c7",
   "metadata": {},
   "source": [
    "```cpp\n",
    "torch::Tensor run_inference(torch::jit::script::Module &module,\n",
    "                            const torch::Tensor &input)\n",
    "{\n",
    "    // TorchScript forward() IValue listesi bekler\n",
    "    std::vector<torch::jit::IValue> inputs;\n",
    "    inputs.push_back(input);\n",
    "\n",
    "    // module.forward() -> IValue, toTensor() ile tensöre çevir\n",
    "    torch::Tensor output = module.forward(inputs).toTensor();\n",
    "    return output;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099e4f6",
   "metadata": {},
   "source": [
    "---\n",
    "# 1) Fonksiyonun amacı\n",
    "\n",
    "Bu fonksiyonun olayı şu:\n",
    "\n",
    "**“Ben bu fonksiyonla, hazır yüklenmiş TorchScript modelime (module) bir giriş tensörü (input) verip, modelden çıkan sonucu tensör olarak geri alıyorum.”**\n",
    "\n",
    "Yani fonksiyon benim için tek iş yapan temiz bir kutu:\n",
    "\n",
    "* İçine: model + input tensör\n",
    "\n",
    "* Dışına: output tensör"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4ac1e",
   "metadata": {},
   "source": [
    "#  2) Parametreler – Neden böyle tanımladım?\n",
    "```cpp\n",
    "torch::jit::script::Module &module,\n",
    "const torch::Tensor &input\n",
    "```\n",
    "\n",
    "\n",
    "### Module &module\n",
    "* Burada ben modülü referans ile alıyorum:\n",
    "\n",
    "* Kopya oluşmasını istemiyorum (model kopyalamak pahalı).\n",
    "\n",
    "* Zaten load ile dışarıda bir kere yükledim, burada sadece onu kullanıyorum.\n",
    "\n",
    "### const torch::Tensor &input\n",
    "Burada da:\n",
    "\n",
    "* Input tensörün kopyasını almak istemiyorum.\n",
    "\n",
    "* Fonksiyon içinde input’u değiştirme niyetim yok.\n",
    "\n",
    "* O yüzden const & ile “okuma amaçlı, kopyasız” kullanıyorum.\n",
    "\n",
    "## Eğer:\n",
    "\n",
    "* Değerle (Module module) alsaydım → her çağrıda model kopyalanırdı, çok gereksiz ve maliyetli olurdu.\n",
    "\n",
    "* input’u kopyalayarak alsaydım → her çağrıda ekstra RAM ve hız kaybı yaşardım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75d80b",
   "metadata": {},
   "source": [
    "# 3) std::vector<torch::jit::IValue> inputs; – Neden IValue?\n",
    "```cpp\n",
    "std::vector<torch::jit::IValue> inputs;\n",
    "inputs.push_back(input);\n",
    "```\n",
    "\n",
    "\n",
    "* TorchScript forward fonksiyonu düz tensör beklemiyor, \n",
    ">std::vector<torch::jit::IValue> bekliyor.\n",
    "\n",
    "### Ben burada şunu yapıyorum:\n",
    "\n",
    "* Bir inputs listesi oluşturuyorum.\n",
    "\n",
    "### inputs.push_back(input); dediğimde:\n",
    "\n",
    "* torch::Tensor otomatik olarak IValue içine sarılıyor.\n",
    "\n",
    "* Yani tensörü “TorchScript’in genel değer tipine” çeviriyorum.\n",
    "\n",
    "## **Neden liste?**\n",
    "\n",
    "**Çünkü TorchScript modeli teorik olarak birden fazla giriş alabilir:**\n",
    "\n",
    "* Örneğin (image, extra_features) gibi.\n",
    "\n",
    "**Benim modelim tek input alıyor ama API genel olduğu için ben yine de vektör kullanmak zorundayım**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae9065",
   "metadata": {},
   "source": [
    "# 4)module.forward(inputs).toTensor() – Burada ne oluyor?\n",
    "```cpp\n",
    "torch::Tensor output = module.forward(inputs).toTensor();\n",
    "```\n",
    "\n",
    "\n",
    "Burada zincir şöyle çalışıyor:\n",
    "\n",
    "### module.forward(inputs)\n",
    "\n",
    "* Ben burada TorchScript modeline forward çağrısı yapıyorum.\n",
    "\n",
    "* Dışarıdan verdiğim inputs listesindeki IValue’ler modele input oluyor.\n",
    "\n",
    "* Geriye yine bir torch::jit::IValue dönüyor (çıktının genel temsili).\n",
    "\n",
    "### .toTensor()\n",
    "\n",
    "* Ben bu IValue’nin aslında tensör olduğunu biliyorum.\n",
    "\n",
    "* O yüzden IValue’yi toTensor() ile tekrar tensöre çeviriyorum.\n",
    "\n",
    ">Sonuç: torch::Tensor output\n",
    "\n",
    "### Eğer -> Modelim aslında tensör değil de tuple, dict vs. döndürseydi:\n",
    "\n",
    "* toTensor() çağırdığım yerde runtime hatası alırdım.\n",
    "\n",
    "### Yanlış input shape verirsem:\n",
    "\n",
    "* Ya forward içinde dimension hatası alırım,\n",
    "\n",
    "* Ya da model saçma çıktı üretir ama yine tensör döner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472172e7",
   "metadata": {},
   "source": [
    "# 5) Bu fonksiyonu böyle yazmasam ne sıkıntı yaşardım?\n",
    "\n",
    "### 1.)Module’ü referans yerine kopya alsam\n",
    "\n",
    "* Her fonksiyon çağrısında model kopyalanır.\n",
    "\n",
    "* Bu hem yavaş hem saçma, özellikle büyük modellerde ciddi performans kaybı.\n",
    "\n",
    "### 2.)Input’u kopyalayarak alsam (const ref yerine)\n",
    "\n",
    "* Her defasında input tensör RAM’de çoğalır.\n",
    "\n",
    "* Büyük görüntülerde gereksiz hafıza ve zaman kaybı.\n",
    "\n",
    "### 3.)IValue vektörü kullanmasam\n",
    "\n",
    "* forward derlenmez, API uymaz.\n",
    "\n",
    "### 4.)toTensor() kullanmasam\n",
    "\n",
    "* Çıkan IValue ile uğraşmam gerekir, her yerde saçma dönüşümler yapmak zorunda kalırım.\n",
    "\n",
    "* Ben çıktı ile tensör seviyesinde çalışmak istediğim için bu kullanım en temiz hali."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6386e",
   "metadata": {},
   "source": [
    "# 6) Ben bu fonksiyonu şöyle özetliyorum\n",
    "\n",
    "**“Ben run_inference fonksiyonunda, TorchScript modelimi kopyalamadan referans olarak alıyorum, input tensörümü IValue listesine sarıp forward’a veriyorum, sonra da dönen IValue’yi tekrar tensöre çevirip çıktıyı tek bir yerden yönetilebilir hale getiriyorum.”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac70859",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d3092",
   "metadata": {},
   "source": [
    "---\n",
    "----\n",
    "----\n",
    "\n",
    "# Şimdi sıra main fonksiyonunda.Unutmayın biz bu açıklamaları şu ana kadar en kapsamlı olan Uygulama - 5 için yapıyoruz.Eğer ilk başta bu .ipynb dosyasını inceliyorsanız , uygulamaları takip etmenizi öneririm.Genel olarak uygulama içerisinde bu adımların açıklamaları mevcuttur.\n",
    "\n",
    "----\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70033046",
   "metadata": {},
   "source": [
    "# Main Fonksiyonu ( int main () )\n",
    "```cpp\n",
    "\n",
    "int main()\n",
    "{\n",
    "    try\n",
    "    {\n",
    "        std::cout << \"[INFO] Program basladi.\\n\";\n",
    "\n",
    "        const std::string model_path = \"model_ts.pt\";\n",
    "\n",
    "        {\n",
    "            std::ifstream f(model_path);\n",
    "            if (!f.good())\n",
    "            {\n",
    "                std::cerr << \"[HATA] model_ts.pt bulunamadi!\\n\";\n",
    "                std::cerr << \"       model_ts.pt dosyasini exe ile ayni klasore koy.\\n\";\n",
    "                std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "                std::cin.get();\n",
    "                return -1;\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "        std::cout << \"[INFO] Model yukleniyor...\\n\";\n",
    "        torch::jit::script::Module module = torch::jit::load(model_path);\n",
    "\n",
    "        // CPU kullanıyoruz (GPU varsa kCUDA da kullanılabilir)\n",
    "        module.to(torch::kCPU);\n",
    "        module.eval();\n",
    "        std::cout << \"[OK] Model yuklendi ve eval modunda.\\n\";\n",
    "\n",
    "\n",
    "        const std::string image_path = \"test.jpg\";\n",
    "        cv::Mat img_bgr = cv::imread(image_path);\n",
    "\n",
    "        if (img_bgr.empty())\n",
    "        {\n",
    "            std::cerr << \"[HATA] Resim yuklenemedi: \" << image_path << \"\\n\";\n",
    "            std::cerr << \"       test.jpg dosyasini exe ile ayni klasore koy.\\n\";\n",
    "            std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "            std::cin.get();\n",
    "            return -1;\n",
    "        }\n",
    "\n",
    "        std::cout << \"[OK] Resim yuklendi. Boyut: \"\n",
    "                  << img_bgr.cols << \"x\" << img_bgr.rows << \" (W x H)\\n\";\n",
    "\n",
    "        int target_size = 64; \n",
    "        torch::Tensor input_tensor = preprocess_image(img_bgr, target_size);\n",
    "\n",
    "        std::cout << \"[DEBUG] Input tensor shape: \" << input_tensor.sizes() << \"\\n\";\n",
    "\n",
    "        // Beklenen shape: [1, 3, 64, 64]\n",
    "        auto sizes = input_tensor.sizes();\n",
    "        if (!(sizes.size() == 4 &&\n",
    "              sizes[0] == 1 &&\n",
    "              sizes[1] == 3 &&\n",
    "              sizes[2] == target_size &&\n",
    "              sizes[3] == target_size))\n",
    "        {\n",
    "            std::cerr << \"[HATA] Input tensor beklenenden farkli! Gelen: \"\n",
    "                      << sizes << \"\\n\";\n",
    "            std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "            std::cin.get();\n",
    "            return -1;\n",
    "        }\n",
    "\n",
    "\n",
    "        std::cout << \"[INFO] Inference calistiriliyor...\\n\";\n",
    "        torch::Tensor output = run_inference(module, input_tensor);\n",
    "\n",
    "        std::cout << \"[DEBUG] Output tensor shape: \" << output.sizes() << \"\\n\";\n",
    "\n",
    "        torch::Tensor probs = torch::softmax(output, 1); \n",
    "        torch::Tensor pred_class = probs.argmax(1);      \n",
    "\n",
    "        std::cout << \"Output (logits): \" << output << \"\\n\";\n",
    "        std::cout << \"Predicted class index: \"\n",
    "                  << pred_class.item<int>() << \"\\n\";\n",
    "\n",
    "        std::cout << \"[INFO] Program bitti. Enter'a basip cikabilirsin...\\n\";\n",
    "        std::cin.get();\n",
    "    }\n",
    "    catch (const c10::Error &e)\n",
    "    {\n",
    "        std::cerr << \"[EXCEPTION - c10] \" << e.what() << \"\\n\";\n",
    "        std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "        std::cin.get();\n",
    "        return -1;\n",
    "    }\n",
    "    catch (const std::exception &e)\n",
    "    {\n",
    "        std::cerr << \"[EXCEPTION - std] \" << e.what() << \"\\n\";\n",
    "        std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "        std::cin.get();\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7abad",
   "metadata": {},
   "source": [
    "## 1️⃣ try–catch yapısı ve giriş logları\n",
    "```cpp\n",
    "int main()\n",
    "{\n",
    "    try\n",
    "    {\n",
    "        std::cout << \"[INFO] Program basladi.\\n\";\n",
    "        ...\n",
    "    }\n",
    "    catch (const c10::Error &e)\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "    catch (const std::exception &e)\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Burada ben şunu yapıyorum:\n",
    "\n",
    "* Tüm main akışını try içine alıyorum.\n",
    "\n",
    "- Eğer LibTorch tarafında bir hata olursa c10::Error yakalıyorum.\n",
    "\n",
    "- Normal C++ istisnaları için std::exception yakalıyorum.\n",
    "\n",
    "- Hata olduğunda mesajı basıp std::cin.get() ile program hemen kapanmasın diye kullanıcıdan Enter bekliyorum.\n",
    "\n",
    "Yani:\n",
    "\n",
    "**“Ben main’i try–catch içine alarak, model yükleme / inference sırasında patlayan hataları kontrollü yakalıyorum ve pencereyi hemen kapatmak yerine kullanıcıya hata mesajını okutuyorum.”**\n",
    "\n",
    "## 2️⃣ Model yolunu belirleme ve dosya kontrolü\n",
    "\n",
    "```cpp\n",
    "const std::string model_path = \"model_ts.pt\";\n",
    "\n",
    "{\n",
    "    std::ifstream f(model_path);\n",
    "    if (!f.good())\n",
    "    {\n",
    "        std::cerr << \"[HATA] model_ts.pt bulunamadi!\\n\";\n",
    "        std::cerr << \"       model_ts.pt dosyasini exe ile ayni klasore koy.\\n\";\n",
    "        std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "        std::cin.get();\n",
    "        return -1;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Burada ben:\n",
    "\n",
    "* model_path ile TorchScript model dosyamın adını sabitliyorum.\n",
    "\n",
    "* std::ifstream ile dosya gerçekten var mı diye kontrol ediyorum.\n",
    "\n",
    "Dosya yoksa:\n",
    "\n",
    "* LibTorch tarafında karmaşık bir hata yerine,\n",
    "\n",
    "* Kullanıcıya net bir mesaj gösteriyorum ve programı temizce sonlandırıyorum.\n",
    "\n",
    "Yani:\n",
    "\n",
    "**“Ben model dosyasının varlığını LibTorch’a bırakmak yerine kendim kontrol ediyorum ki, kullanıcı insanca bir hata mesajı görsün.”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d21136",
   "metadata": {},
   "source": [
    "## 3️⃣ TorchScript modelini yükleme ve hazırlama\n",
    "```cpp\n",
    "std::cout << \"[INFO] Model yukleniyor...\\n\";\n",
    "torch::jit::script::Module module = torch::jit::load(model_path);\n",
    "\n",
    "// CPU kullanıyoruz (GPU varsa kCUDA da kullanılabilir)\n",
    "module.to(torch::kCPU);\n",
    "module.eval();\n",
    "std::cout << \"[OK] Model yuklendi ve eval modunda.\\n\";\n",
    "```\n",
    "\n",
    "Burada :\n",
    "\n",
    "* torch::jit::load ile .pt dosyasından TorchScript modeli yüklüyorum.\n",
    "\n",
    "* module.to(torch::kCPU); ile modeli CPU üzerine taşıyorum.\n",
    "\n",
    "### module.eval(); ile modeli evaluation moduna alıyorum:\n",
    "\n",
    "* Dropout vs. gibi eğitimde davranan katmanlar inference moduna geçiyor.\n",
    "\n",
    "* Log’larla da adımları takip edilebilir hale getiriyorum.\n",
    "\n",
    "### Yapmazsam ne olur?\n",
    "\n",
    "#### eval() yapmazsam:\n",
    "\n",
    "* Eğitim davranışı devam eder, özellikle dropout/BatchNorm gibi katmanlar tutarsız sonuç verebilir.\n",
    "\n",
    "#### to(torch::kCPU) koymazsam:\n",
    "\n",
    "*  Varsayılan genelde CPU olur ama ileride GPU kullanırsam karışıklık çıkar; explicit yazmak kontrolü bana veriyor.\n",
    "\n",
    "## 4️⃣ Test görselini okuma\n",
    "```cpp\n",
    "const std::string image_path = \"test.jpg\";\n",
    "cv::Mat img_bgr = cv::imread(image_path);\n",
    "\n",
    "if (img_bgr.empty())\n",
    "{\n",
    "    std::cerr << \"[HATA] Resim yuklenemedi: \" << image_path << \"\\n\";\n",
    "    std::cerr << \"       test.jpg dosyasini exe ile ayni klasore koy.\\n\";\n",
    "    std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "    std::cin.get();\n",
    "    return -1;\n",
    "}\n",
    "\n",
    "std::cout << \"[OK] Resim yuklendi. Boyut: \"\n",
    "          << img_bgr.cols << \"x\" << img_bgr.rows << \" (W x H)\\n\";\n",
    "```\n",
    "\n",
    "Burada :\n",
    "\n",
    "* cv::imread ile resmi BGR formatında yüklüyorum.\n",
    "\n",
    "* img_bgr.empty() ile dosya gerçekten okunmuş mı kontrol ediyorum.\n",
    "\n",
    "* Hata durumunda yine kullanıcıya düzgün mesaj verip çıkıyorum.\n",
    "\n",
    "* Başarılıysa, resmin genişlik–yükseklik bilgisini logluyorum.\n",
    "\n",
    "## 5️⃣ Preprocess çağrısı ve input tensör shape kontrolü\n",
    "```cpp\n",
    "int target_size = 64; // Python tarafında Resize((64,64)) kullandık\n",
    "torch::Tensor input_tensor = preprocess_image(img_bgr, target_size);\n",
    "\n",
    "std::cout << \"[DEBUG] Input tensor shape: \" << input_tensor.sizes() << \"\\n\";\n",
    "\n",
    "auto sizes = input_tensor.sizes();\n",
    "if (!(sizes.size() == 4 &&\n",
    "      sizes[0] == 1 &&\n",
    "      sizes[1] == 3 &&\n",
    "      sizes[2] == target_size &&\n",
    "      sizes[3] == target_size))\n",
    "{\n",
    "    std::cerr << \"[HATA] Input tensor beklenenden farkli! Gelen: \"\n",
    "              << sizes << \"\\n\";\n",
    "    std::cout << \"Enter'a basip cikabilirsin...\\n\";\n",
    "    std::cin.get();\n",
    "    return -1;\n",
    "}\n",
    "```\n",
    "Burada :\n",
    "\n",
    "### preprocess_image ile:\n",
    "\n",
    "* Resize\n",
    "\n",
    "* BGR → RGB\n",
    "\n",
    "* Normalize\n",
    "\n",
    "* cv::Mat → torch::Tensor\n",
    "\n",
    "işlemlerini tek fonksiyonda topluyorum.\n",
    "\n",
    "**Sonra tensor shape’ini sizes() ile alıp kontrol ediyorum:**\n",
    "\n",
    "* 4 boyutlu mu?\n",
    "\n",
    "* Batch = 1 mi?\n",
    "\n",
    "* Kanal = 3 mü?\n",
    "\n",
    "* Yükseklik/genişlik = 64 mü?\n",
    "\n",
    "#### Bunu yapmamın sebebi:\n",
    "\n",
    "* “Ben input tensörün modelle birebir uyumlu olduğunu garanti altına alıyorum. Aksi halde debug çok zorlaşır.”\n",
    "\n",
    "### Yapmazsam:\n",
    "\n",
    "* Yanlış shape modelin içinde patlar.\n",
    "\n",
    "* Hata mesajı daha karışık olur (conv içinde dimension mismatch vs.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360cfe2",
   "metadata": {},
   "source": [
    "## 6️⃣ Inference çalıştırma\n",
    "```cpp\n",
    "std::cout << \"[INFO] Inference calistiriliyor...\\n\";\n",
    "torch::Tensor output = run_inference(module, input_tensor);\n",
    "\n",
    "std::cout << \"[DEBUG] Output tensor shape: \" << output.sizes() << \"\\n\";\n",
    "```\n",
    "\n",
    "Burada:\n",
    "\n",
    "* run_inference fonksiyonunu çağırarak modeli input tensörle çalıştırıyorum.\n",
    "\n",
    "* Çıkan tensörün shape’ini debug için yazdırıyorum.\n",
    "\n",
    "* Genelde [1, num_classes] bekliyorum (örneğin [1, 10]).\n",
    "\n",
    "### Bu ayrı fonksiyon yapısının bana faydası:\n",
    "\n",
    "**“Ben forward çağrısını main’den soyutladım. Böylece inference akışını ayrı bir fonksiyon içinde temizce yönetebiliyorum.”**\n",
    "\n",
    "## 7️⃣ Postprocess: softmax + argmax + item\n",
    "```cpp\n",
    "torch::Tensor probs = torch::softmax(output, 1); // sınıf olasılıkları\n",
    "torch::Tensor pred_class = probs.argmax(1);      // en yüksek olasılıklı sınıf index'i\n",
    "\n",
    "std::cout << \"Output (logits): \" << output << \"\\n\";\n",
    "std::cout << \"Predicted class index: \"\n",
    "          << pred_class.item<int>() << \"\\n\";\n",
    "```\n",
    "\n",
    "Burada :\n",
    "\n",
    "* output tensörünü softmax ile olasılığa çeviriyorum.\n",
    "\n",
    "* argmax(1) ile en yüksek olasılıklı sınıfın indeksini buluyorum.\n",
    "\n",
    "* itemint>() ile bu index’i C++ tarafında normal bir int olarak alıyorum.\n",
    "\n",
    "* Hem ham logits değerlerini hem de tahmin edilen sınıf indeksini logluyorum.\n",
    "\n",
    "### Yapmazsam:\n",
    "\n",
    "**Softmax kullanmadan da argmax yapabilirim; ama:**\n",
    "\n",
    "* Olasılık dağılımını görmek istesem softmax’a ihtiyaç var.\n",
    "\n",
    "* Eğitim–inference tutarlılığı açısından softmax mantıklı.\n",
    "\n",
    "## 8️⃣ Program sonu: Enter bekleme\n",
    "```cpp\n",
    "std::cout << \"[INFO] Program bitti. Enter'a basip cikabilirsin...\\n\";\n",
    "std::cin.get();\n",
    "```\n",
    "\n",
    "Burada da:\n",
    "\n",
    "**“Ben program bittiğinde konsol penceresinin anında kapanmaması için kullanıcıdan Enter bekliyorum, böylece logları rahatça görebiliyorum.”**\n",
    "\n",
    "## 9️⃣ catch blokları – Hata yakalama stratejisi\n",
    "```cpp\n",
    "catch (const c10::Error &e)\n",
    "{\n",
    "    std::cerr << \"[EXCEPTION - c10] \" << e.what() << \"\\n\";\n",
    "    ...\n",
    "}\n",
    "catch (const std::exception &e)\n",
    "{\n",
    "    std::cerr << \"[EXCEPTION - std] \" << e.what() << \"\\n\";\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Burada:\n",
    "\n",
    "* LibTorch kaynaklı hatalar için c10::Error ayrı tutuluyor.\n",
    "\n",
    "* Diğer standart C++ hataları için std::exception.\n",
    "\n",
    "* İkisinde de kullanıcıya mesaj gösterip Enter bekliyorum.\n",
    "\n",
    "Bu sayede:\n",
    "\n",
    "**“Ben hem LibTorch hem de genel C++ hatalarını ayrı ayrı kategorize edebiliyorum ve debug ederken hangi tarağın patladığını net görebiliyorum.”**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549beb9",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
